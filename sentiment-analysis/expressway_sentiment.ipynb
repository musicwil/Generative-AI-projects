{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-kfSgIGzgGL"
      },
      "source": [
        "# **Project : A Case Study of ExpressWay Logistics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8Z-Vp4pzgGN"
      },
      "source": [
        "**Business Overview:**\n",
        "\n",
        "ExpressWay Logistics is a dynamic logistics service provider, committed to delivering efficient, reliable and cost-effective courier transportation and warehousing solutions. With a focus on speed, precision and customer satisfaction, we aim to be the go-to partner for our customers seeking seamless courier services. Our core service involves ensuring operational efficiency throughout our delivery and courier services, including inventory management, durable packaging and swift dispatch of couriers, real time tracking of shipments and on-time delivery of couriers as promised. We are committed to enhance our logistics and courier services and improve seamless connectivity for our customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46XFRcFTzgGN"
      },
      "source": [
        "**Current Challenge:**\n",
        "\n",
        "ExpressWay Logistics faces numerous challenges in ensuring seamless deliveries and customer satisfaction. These challenges include managing various customer demands simultaneously, addressing delays in deliveries and ensuring products arrive intact and safe. Additionally, the company struggles with complexity of efficiently storing and handling a large volume of packages and ultimately meeting customer expectations. Moreover, maintaining a skilled workforce capable of handling various aspects of logistics operations presents its own set of challenges. Overcoming these obstacles requires a comprehensive approach that integrates innovative technology, strategic planning, and continuous improvement initiatives to ensure smooth operations and exceptional service delivery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maC1EgUJzgGO"
      },
      "source": [
        "**Objective:**\n",
        "\n",
        "Our primary objective is to conduct a sentiment analysis of user-generated reviews across various digital channels and platforms. By paying attention to their feedback, we want to find ways to make our services better - like handling different customer demands simultaneously, dealing with late deliveries, and keeping packages secured and intact. Through the application of prompt engineering methodologies and sentiment analysis, we'll figure out if sentiments expressed by users for our courier services are Positive or Negative. This will help us understand where we need to improve in order to meet customer expectations and keep them happy. With a focus on getting better all the time, we'll overcome the challenges at ExpressWay Logistics and make our services the best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flUHF3xczgGO"
      },
      "source": [
        "**Data Description:**\n",
        "\n",
        "The dataset titled \"courier-service_reviews.csv\" is structured to facilitate sentiment analysis for courier service reviews. Here's a brief description of the data columns:\n",
        "\n",
        "1. id: This column contains unique identifiers for each review entry. It helps in distinguishing and referencing individual reviews.\n",
        "2. review: This column includes the actual text of the courier service reviews. The reviews are likely composed of customer opinions and experiences regarding different aspects of the services provided by ExpressWay Logistics.\n",
        "3. sentiment: This column provides an additional layer of classification (positive and negative) for the mentioned reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zTC8qPRPahk"
      },
      "source": [
        "##**Step 1. Setup**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykLYC3zzSr8z"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keb94_PFzgGO"
      },
      "outputs": [],
      "source": [
        "# Installing necessary packages from openai (LLM), tiktoken (token count)\n",
        "the\n",
        "\n",
        "!pip install -q openai==1.55.3 tiktoken==0.6.0 session-info --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFZj9kr8Pahl"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3gwxSqQPahl"
      },
      "outputs": [],
      "source": [
        "# Import all Python packages required to access the Azure Open AI API.\n",
        "# Import additional packages required to access datasets and create examples.\n",
        "\n",
        "from openai import AzureOpenAI\n",
        "import json\n",
        "import random\n",
        "import tiktoken\n",
        "import session_info\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mmXtfmrS7Ml"
      },
      "outputs": [],
      "source": [
        "#session_info.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN0wIDUjPahn"
      },
      "source": [
        "### Authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvDZdCMhzgGQ"
      },
      "source": [
        "**(A) Writing/Creating the config.json file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO4TwVYHzgGQ"
      },
      "outputs": [],
      "source": [
        "# Creating variable data for the azure credentials\n",
        "\n",
        "with open('config.json', 'r') as az_creds:\n",
        "    data = az_creds.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL9GpBEVPahn"
      },
      "outputs": [],
      "source": [
        "# Converting it to a json dictionary\n",
        "\n",
        "creds = json.loads(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud7bPDquPahp"
      },
      "outputs": [],
      "source": [
        "# Creating an Azure OpenAI instance\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint=creds[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    api_key=creds[\"AZURE_OPENAI_KEY\"],\n",
        "    api_version=creds[\"AZURE_OPENAI_APIVERSION\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Atf3l2mNPahp"
      },
      "outputs": [],
      "source": [
        "# Naming a variable to read chatgpt model\n",
        "\n",
        "chat_model_id = creds[\"CHATGPT_MODEL\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQpkd5elTMhd"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function calculates the number of tokens used in a list of messages, which is useful for estimating API usage costs in OpenAI models. It first sets up token encoding based on the gpt-4 model. Each message is counted with an overhead of three tokens due to the special formatting (<|start|>, role (system, user, or assistant), and <|end|>). It then iterates through the messages, encoding and counting tokens for each key-value pair. Finally, an additional three tokens are added to account for the assistant's reply formatting."
      ],
      "metadata": {
        "id": "CWmiNrVPv4Oj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5Gp-_CxPahp"
      },
      "outputs": [],
      "source": [
        "# Defining a function to evaluate the token consumption per model for cost estimating\n",
        "\n",
        "def num_tokens_from_messages(messages):\n",
        "\n",
        "    \"\"\"\n",
        "    Return the number of tokens used by a list of messages.\n",
        "    Adapted from the Open AI cookbook token counter\n",
        "    \"\"\"\n",
        "\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "\n",
        "    # Each message is sandwiched with <|start|>role and <|end|>\n",
        "    # Hence, messages look like: <|start|>system or user or assistant{message}<|end|>\n",
        "\n",
        "    tokens_per_message = 3 # token1:<|start|>, token2:system(or user or assistant), token3:<|end|>\n",
        "\n",
        "    num_tokens = 0\n",
        "\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "\n",
        "    return num_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sP7s5TDPahq"
      },
      "source": [
        "## Task : Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhctrBwFPahq"
      },
      "source": [
        "##**Step 2: Assemble Data**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLK-296WzgGS"
      },
      "source": [
        "**(A) Upload and read csv file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SA5Cyqgk5kgB"
      },
      "outputs": [],
      "source": [
        "# Reading CSV File with dataset into a dataframe\n",
        "\n",
        "cs_reviews_df = pd.read_csv(\"courier_service_review.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-NSh5_p7V5E"
      },
      "outputs": [],
      "source": [
        "# Checking variables, data types, missing values, and size of dataset\n",
        "\n",
        "cs_reviews_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJmr1iJ5_tNm"
      },
      "outputs": [],
      "source": [
        "# Printing some examples to get to know the data\n",
        "\n",
        "cs_reviews_df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(B) Count Positive and Negative Sentiment Reviews**"
      ],
      "metadata": {
        "id": "CKUDLCbC0bfB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5AbhvRV9o-e"
      },
      "outputs": [],
      "source": [
        "# Verifying if label is skewed\n",
        "\n",
        "cs_reviews_df['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3Z0q0b6zgGT"
      },
      "outputs": [],
      "source": [
        "# Checking the number of rows and columns\n",
        "\n",
        "cs_reviews_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiXuxmchzgGT"
      },
      "source": [
        "**(C) Split the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkXtXbc9N5rC"
      },
      "outputs": [],
      "source": [
        "# Splitting the data with a robust test set to enhance performance for good predictions\n",
        "\n",
        "cs_examples_df, cs_gold_examples_df = train_test_split(\n",
        "    cs_reviews_df,\n",
        "    test_size=0.40, # 40% random sample selected for gold examples - increased from inital 20%\n",
        "    random_state=42 # ensures that the splits are the same for every session\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGSj2dfZOMW-"
      },
      "outputs": [],
      "source": [
        "# Verifying size of train and test sets\n",
        "\n",
        "(cs_examples_df.shape, cs_gold_examples_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ExVGFPl4tdx"
      },
      "source": [
        "To select gold examples for this session, we sample randomly from the test data using a `random_state=42`. This ensures that the examples from multiple runs of the sampling are the same (i.e., they are randomly selected but do not change between different runs of the notebook). Note that we are doing this only to keep execution times low for illustration. In practise, large number of gold examples facilitate robust estimates of model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLj4LVu_Ozc_"
      },
      "outputs": [],
      "source": [
        "# Selecting columns to extract values for gold_examples\n",
        "\n",
        "columns_to_select = ['review','sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDIZm2RdMsDt"
      },
      "outputs": [],
      "source": [
        "# Picking 53 samples for evaluation purpose. Examples will not change.\n",
        "# They will only be sorted out randomly. Also, to convert gold_examples to a Json dictionary\n",
        "\n",
        "gold_examples = (\n",
        "        cs_gold_examples_df.loc[:, columns_to_select]\n",
        "                                     .sample(53, random_state=42) #<- ensures that gold examples are the same for every session\n",
        "                                     .to_json(orient='records')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AcH3eqgzgGV"
      },
      "outputs": [],
      "source": [
        "# Checking the gold examples\n",
        "\n",
        "gold_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8J58KMMM95s"
      },
      "outputs": [],
      "source": [
        "# To convert gold_examples to a dictionary in Json format. Verifying format with one sample.\n",
        "\n",
        "json.loads(gold_examples)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPsU-h8FPaht"
      },
      "source": [
        "##**Step 3: Derive Prompt**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfDUKbCgPahu"
      },
      "outputs": [],
      "source": [
        "# Establishing user input message\n",
        "\n",
        "user_message_template = \"\"\"```{courier_service_review}```\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3uS8NWfNqBG"
      },
      "source": [
        "**(A) Write Zero Shot System Message**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J79_IaxHJdY2"
      },
      "outputs": [],
      "source": [
        "# Writing zero shot system message\n",
        "\n",
        "zero_shot_system_message = \"\"\"\n",
        "Classify customer review in the input as positive or negative in sentiment.\n",
        "Reviews will be delimited by triple backticks, that is, ```.\n",
        "Do not explain your answer. Your answer should only contain the label: positive or negative.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46AWalvrLTxs"
      },
      "source": [
        "**(B) Create Zero Shot Prompt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8cjNjiJJpzc"
      },
      "outputs": [],
      "source": [
        "# Creating zero shot prompt to be input ready for completion function\n",
        "\n",
        "zero_shot_prompt = [\n",
        "    {\"role\": \"system\", \"content\": zero_shot_system_message},\n",
        "    {\"role\": \"user\", \"content\": user_message_template}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlbBt1u_OJEz"
      },
      "outputs": [],
      "source": [
        "# Getting the token consumption for the zero shot prompt\n",
        "\n",
        "num_tokens_from_messages(zero_shot_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(C) Write Few Shot System Message**"
      ],
      "metadata": {
        "id": "48bF42gAyQ7Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nrW4jzGLdce"
      },
      "outputs": [],
      "source": [
        "# Writing few shot system message\n",
        "\n",
        "few_shot_system_message = \"\"\"\n",
        "Classify customer review in the input as positive or negative in sentiment.\n",
        "Reviews will be delimited by triple backticks, that is, ```.\n",
        "Do not explain your answer. Your answer should only contain the label: positive or negative.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3ExNtgPRDhJ"
      },
      "source": [
        "Merely selecting random samples from the polarity subsets is not enough because the examples included in a prompt are prone to a set of known biases such as:\n",
        " - Majority label bias (frequent answers in predictions)\n",
        " - Recency bias (examples near the end of the prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEA_Ab2FRrYB"
      },
      "source": [
        "To avoid these biases, it is important to have a balanced set of examples that are arranged in random order. Let us create a Python function that generates bias-free examples:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```create_examples``` function generates a randomized set of example reviews with equal representation from two sentiment classes: Positive and Negative. It first filters the dataset into two separate groups based on sentiment labels. Then, it randomly selects `n` examples from each class and combines them into a single dataset. The combined examples are shuffled to ensure randomness before being converted into a JSON format. Each time the function runs, it produces a different set of randomized examples from the dataset."
      ],
      "metadata": {
        "id": "6GwlhupAx_IM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb0mLtE9R7ZA"
      },
      "outputs": [],
      "source": [
        "# Function to create examples. See description above. (For higher efficiency, I'm using n=16)\n",
        "\n",
        "def create_examples(dataset, n=16):\n",
        "\n",
        "    \"\"\"\n",
        "    Return a JSON list of randomized examples of size 2n with two classes.\n",
        "    Create subsets of each class, choose random samples from the subsets,\n",
        "    merge and randomize the order of samples in the merged list.\n",
        "    Each run of this function creates a different random sample of examples\n",
        "    chosen from the training data.\n",
        "\n",
        "    Args:\n",
        "        dataset (DataFrame): A DataFrame with examples (review + label)\n",
        "        n (int): number of examples of each class to be selected\n",
        "\n",
        "    Output:\n",
        "        randomized_examples (JSON): A JSON with examples in random order\n",
        "    \"\"\"\n",
        "\n",
        "    positive_reviews = (dataset.sentiment == 'Positive')\n",
        "    negative_reviews = (dataset.sentiment == 'Negative')\n",
        "    columns_to_select = ['review', 'sentiment']\n",
        "\n",
        "    positive_examples = dataset.loc[positive_reviews, columns_to_select].sample(n)\n",
        "    negative_examples = dataset.loc[negative_reviews, columns_to_select].sample(n)\n",
        "\n",
        "    examples = pd.concat([positive_examples, negative_examples])\n",
        "\n",
        "    # sampling without replacement is equivalent to random shuffling\n",
        "\n",
        "    randomized_examples = examples.sample(2*n, replace=False)\n",
        "\n",
        "    return randomized_examples.to_json(orient='records')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(D) Create Examples For Few shot prompt**"
      ],
      "metadata": {
        "id": "vHhPExVTyO93"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb1VpSgZPaht"
      },
      "outputs": [],
      "source": [
        "# Creating the examples (2n=32, 16 per class)\n",
        "\n",
        "examples = create_examples(cs_reviews_df, n=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJInVW2PSucc"
      },
      "outputs": [],
      "source": [
        "# Converting examples to Json format\n",
        "\n",
        "json.loads(examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEnNkOzJSwTk"
      },
      "source": [
        "With the examples in place, we can now assemble a few-shot prompt. Since we will be using the few-shot prompt several times during evaluation, let us write a function to create a few-shot prompt (the logic of this function is depicted below)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create_prompt function constructs a few-shot prompt formatted for the OpenAI API, incorporating system instructions, example interactions, and a user message template. It starts by adding the system message, which provides guidelines for sentiment analysis. Then, it loops through the provided examples, formatting each review as a user message and its corresponding sentiment as an assistant response. These are appended to the prompt list in the required sequence. The final output is a structured list of dictionaries that can be directly used as input for an OpenAI model."
      ],
      "metadata": {
        "id": "Yzw31kh8yjb4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFK2WQOgS8ey"
      },
      "outputs": [],
      "source": [
        "# Defining function to create few_shot_prompt\n",
        "\n",
        "def create_prompt(system_message, examples, user_message_template):\n",
        "\n",
        "    \"\"\"\n",
        "    Return a prompt message in the format expected by the Open AI API.\n",
        "    Loop through the examples and parse them as user message and assistant\n",
        "    message.\n",
        "\n",
        "    Args:\n",
        "        system_message (str): system message with instructions for sentiment analysis\n",
        "        examples (str): JSON string with list of examples\n",
        "        user_message_template (str): string with a placeholder for courier service reviews\n",
        "\n",
        "    Output:\n",
        "        few_shot_prompt (List): A list of dictionaries in the Open AI prompt format\n",
        "    \"\"\"\n",
        "\n",
        "    few_shot_prompt = [{'role':'system', 'content': system_message}]\n",
        "\n",
        "    for example in json.loads(examples):\n",
        "        example_review = example['review']\n",
        "        example_sentiment = example['sentiment']\n",
        "\n",
        "        few_shot_prompt.append(\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': user_message_template.format(\n",
        "                    courier_service_review=example_review\n",
        "                )\n",
        "            }\n",
        "        )\n",
        "\n",
        "        few_shot_prompt.append(\n",
        "            {'role': 'assistant', 'content': f\"{example_sentiment}\"}\n",
        "        )\n",
        "\n",
        "    return few_shot_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(E) Create Few Shot Prompt**"
      ],
      "metadata": {
        "id": "5YN0Zt1dyZ3X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoO1lCFtTBUV"
      },
      "outputs": [],
      "source": [
        "# Creating Few shot prompt\n",
        "\n",
        "few_shot_prompt = create_prompt(\n",
        "    few_shot_system_message,\n",
        "    examples,\n",
        "    user_message_template\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy91eq9FTNeo"
      },
      "outputs": [],
      "source": [
        "# Verifying content for executed few shot prompt\n",
        "\n",
        "few_shot_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItFH6zLJMnb9"
      },
      "outputs": [],
      "source": [
        "# Getting the token count for few_shot_prompt\n",
        "\n",
        "num_tokens_from_messages(few_shot_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcI8oyIcPfPW"
      },
      "source": [
        "##**Step 4: Evaluate prompts**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BWfjbiFUCSp"
      },
      "source": [
        "Now we have two sets of prompts that we need to evaluate using gold labels. Since the few-shot prompt depends on the sample of examples that was drawn to make up the prompt, we expect some variability in evaluation. Hence, we evaluate each prompt multiple times to get a sense of the average and the variation around the average.\n",
        "\n",
        "To reiterate, a choice on the prompt should account for variability due to the choice of the random sample. To aid repeated evaluation, we assemble an evaluation function ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ```evaluate_prompt``` function evaluates the performance of a sentiment analysis model using a micro-F1 score by comparing its predictions to gold-standard examples. It iterates through gold examples, formats each review into a user input message, and appends it to the provided prompt. The prompt is then sent to the OpenAI model for prediction, ensuring deterministic outputs by setting a low temperature and restricting the token limit. The predicted sentiments are collected alongside ground truth labels for evaluation. Finally, the function calculates and prints the micro-F1 score while displaying a comparison table of reviews, predictions, and actual labels.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p1zDjL3-0Kr0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWpWWcLAzgGY"
      },
      "outputs": [],
      "source": [
        "# Defining function to evaluate both prompting techniques\n",
        "\n",
        "def evaluate_prompt(prompt, gold_examples, user_message_template):\n",
        "\n",
        "    \"\"\"\n",
        "    Return the micro-F1 score for predictions on gold examples.\n",
        "    For each example, we make a prediction using the prompt. Gold labels and\n",
        "    model predictions are aggregated into lists and compared to compute the\n",
        "    F1 score.\n",
        "\n",
        "    Args:\n",
        "        prompt (List): list of messages in the Open AI prompt format\n",
        "        gold_examples (str): JSON string with list of gold examples\n",
        "        user_message_template (str): string with a placeholder for courier service review\n",
        "\n",
        "    Output:\n",
        "        micro_f1_score (float): Micro-F1 score computed by comparing model predictions\n",
        "                                with ground truth\n",
        "    \"\"\"\n",
        "\n",
        "    model_predictions, ground_truths, review_texts = [], [], []\n",
        "\n",
        "    for example in json.loads(gold_examples):\n",
        "        gold_input = example['review']\n",
        "        user_input = [\n",
        "            {\n",
        "                'role':'user',\n",
        "                'content': user_message_template.format(courier_service_review=gold_input)\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=chat_model_id,\n",
        "                messages=prompt+user_input,\n",
        "                temperature=0, # <- Note the low temperature (For a deterministic response)\n",
        "                max_tokens=2 # <- Note how we restrict the output to not more than 2 tokens\n",
        "            )\n",
        "\n",
        "            prediction = response.choices[0].message.content\n",
        "\n",
        "            model_predictions.append(prediction.strip().lower()) # <- removes extraneous white spaces & sets all in lower case\n",
        "            ground_truths.append(example['sentiment'].lower())\n",
        "            review_texts.append(gold_input)\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    micro_f1_score = f1_score(ground_truths, model_predictions, average=\"micro\")\n",
        "\n",
        "    table_data = [[text, pred, truth] for text, pred, truth in zip(review_texts, model_predictions, ground_truths)]\n",
        "    headers = [\"Review\", \"Model Prediction\", \"Ground Truth\"]\n",
        "    print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
        "\n",
        "    return micro_f1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s61rE4tMVHqt"
      },
      "source": [
        "Let us now use this function to do one evaluation of all the two prompts assembled so far, each time computing the Micro-F1 score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpmEiVPIZUBl",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**(A) Evaluate zero shot prompt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kar164F4nd9x"
      },
      "outputs": [],
      "source": [
        "# Executing function to evaluate zero shot prompt\n",
        "\n",
        "evaluate_prompt(zero_shot_prompt, gold_examples, user_message_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya73XdatZUBl",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**(B) Evaluate few shot prompt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqQTgKUkZUBl",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Executing function to evaluate few_shot_prompt\n",
        "\n",
        "evaluate_prompt(few_shot_prompt, gold_examples, user_message_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUYn7f-LVzs-"
      },
      "source": [
        "However, this is just *one* choice of examples. We will need to run these evaluations with multiple choices of examples to get a sense of variability in F1 score for the few-shot prompt. As an example, let us run evaluations for the few-shot prompt 5 times (increased to 10 in final run)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part evaluates the variability of the micro-F1 score for few-shot and zero-shot prompts by running multiple evaluations with different example selections. It loops num_eval_runs times, generating a new random set of examples in each iteration. A zero-shot prompt is created using only system instructions, while a few-shot prompt includes both system instructions and the selected examples. Each prompt is then evaluated on gold-standard examples to measure performance. The resulting micro-F1 scores for both methods are stored in separate lists for further analysis.\n"
      ],
      "metadata": {
        "id": "exqfKeaz0xeK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTXUDXYy7dku"
      },
      "outputs": [],
      "source": [
        " # Iterating on data to check F-1 score variability to compare prompts\n",
        "\n",
        "num_eval_runs =10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6pQkocytqG_"
      },
      "outputs": [],
      "source": [
        "# Storing results in lists\n",
        "\n",
        "zero_shot_performance = []\n",
        "few_shot_performance = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdUaodX3tseE"
      },
      "outputs": [],
      "source": [
        "# Running the evaluations\n",
        "\n",
        "for _ in tqdm(range(num_eval_runs)):\n",
        "\n",
        "    # For each run create a new sample of examples\n",
        "    examples = create_examples(cs_examples_df)\n",
        "\n",
        "    # Assemble the zero shot prompt with these examples\n",
        "    zero_shot_prompt = [{'role':'system', 'content': zero_shot_system_message}]\n",
        "\n",
        "    # Assemble the few shot prompt with these examples\n",
        "    few_shot_prompt = create_prompt(few_shot_system_message, examples, user_message_template)\n",
        "\n",
        "    # Evaluate zero shot prompt accuracy on gold examples\n",
        "    zero_shot_micro_f1 = evaluate_prompt(zero_shot_prompt, gold_examples, user_message_template)\n",
        "\n",
        "    # Evaluate few shot prompt accuracy on gold examples\n",
        "    few_shot_micro_f1 = evaluate_prompt(few_shot_prompt, gold_examples, user_message_template)\n",
        "\n",
        "    zero_shot_performance.append(zero_shot_micro_f1)\n",
        "    few_shot_performance.append(few_shot_micro_f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JSJzAeZZUBl",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**(C) Calculate Mean and Standard Deviation for Zero Shot Prompt and Few Shot Prompt**\n",
        "\n",
        "Compute the average (mean) and measure the variability (standard deviation) of the evaluation scores for both zero shot and few shot prompts."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating mean and standard deviation for F-1 micro score for Zero Shot Prompt\n",
        "\n",
        "np.array(zero_shot_performance).mean(), np.array(zero_shot_performance).std()"
      ],
      "metadata": {
        "id": "Wlx7hRekacH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating mean and standard deviation for F-1 micro score for Fex Shot Prompt\n",
        "\n",
        "np.array(few_shot_performance).mean(), np.array(few_shot_performance).std()"
      ],
      "metadata": {
        "id": "aUt85F--aehR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 5: Observation and Insights and Business perspective**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P5bxwSR894YB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5.1 Observations on the model**"
      ],
      "metadata": {
        "id": "syp8QvPVYOOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As we have learned throught this course, Prompt Engeneering is an iterative process. And this was certainly the case with ExpressWay Logistics case.\n",
        "- After an inital review of the notebook and run of the project we obtained some scores.These inital score results demanded further action as they did not look promising: same scores for both techniques, and 0.0 standard deviations. A call for further research was required.\n",
        "- I decided to sequentially apply a Data Science common practice: the more datapoints we have, the better results we should obtain. I started to methodically modify certain snippets.\n",
        "- Moreover, a comment we have seen in more than one notebook throughout the course coincides with this principle: \"In practice, large number of gold examples facilitate robust estimates of model accuracy.\"\n",
        "- My first adjustment was to the test_size. I increased it to 40%, which resulted in 53 (from an initial 27) data points for the test_size set, that is, cs_gold_examples_df. The gold examples sample looks more robust now.\n",
        "- As a general practice, to see the effect of this change I executed the notebook up to the final scores. There were improvement in the scores, but they still remained the same for both prompting techniques. This meant that there was still more work to do.\n",
        "- The rule of large numbers had to be applied to the other numeric components of the task on hand. I proceeded to increase the number of examples from 4(8) to 16(32). Individual results were better, but the samples were still not enough to create a more distinguishable difference between the two prompts.\n",
        "- Then I extended the number of evaluation runs from 5 to 10. And the rule of large numbers was proven right.\n",
        "- Mathematically, the zero-shot's superior performance is evidenced by its higher mean F1-score (0.962) and negligible standard deviation, ensuring a consistently high prediction accuracy, while the few-shot's lower mean F1-score (0.960) and noticeable standard deviation (0.00566) suggest less predictable and slightly poorer average outcomes. Both scores were good, but zero_shot offered a more powerful predictability capacity.\n",
        "- This, coupled with the intuitive lower token cost, logically leads to the conclusion that the zero-shot prompt is the winner of this task.\n"
      ],
      "metadata": {
        "id": "LYL-XuN8N1wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5.2 Insights from sentiment analysis and its impact on the business**"
      ],
      "metadata": {
        "id": "5k8f-e9wN1Xt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.2.1 Customer dissatisfaction level**"
      ],
      "metadata": {
        "id": "i8rpDuANp5T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Percentage of dissatisfied customers as per review reviews data is {63/131*100:.0f}%\")"
      ],
      "metadata": {
        "id": "WSy0503wcgFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It is clear, from the sample of 131 courier reviews from customers, that there is a high level of unsatisfied customer"
      ],
      "metadata": {
        "id": "BuVj2WN_N1Kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.2.2 Top 5 customer complaints**"
      ],
      "metadata": {
        "id": "c57VPLnmqN8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find out the top 5 complaints from the courier_service_review dataset"
      ],
      "metadata": {
        "id": "4hnvLJk4eXkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a list of negative reviews\n",
        "\n",
        "negative_review_texts = cs_reviews_df[cs_reviews_df['sentiment'] == 'Negative']['review'].tolist()"
      ],
      "metadata": {
        "id": "GtV_vSTeiuoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(negative_review_texts)"
      ],
      "metadata": {
        "id": "TETMOh7Bi3D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking a sample of the output\n",
        "\n",
        "negative_review_texts[0]"
      ],
      "metadata": {
        "id": "BxRKE_kYjfsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complaint_extraction_system_message = \"\"\"\n",
        "You are an AI assistant specialized in analyzing customer feedback. Your task is to\n",
        "identify and list the most common complaints found in the provided courier service\n",
        "reviews. Summarize them concisely.\n",
        "\"\"\"\n",
        "\n",
        "# to concatenate negaative reviews ibnto one single string\n",
        "combined_negative_reviews = \"\\n\".join(negative_review_texts[:63])\n",
        "\n",
        "complaint_extraction_user_message = f\"\"\"Here are several negative courier service reviews:\n",
        "\n",
        "{combined_negative_reviews}\n",
        "\n",
        "Based on these reviews, what are the 5 most common complaints? List them concisely.\"\"\"\n",
        "\n",
        "messages_for_complaints = [\n",
        "    {\"role\": \"system\", \"content\": complaint_extraction_system_message},\n",
        "    {\"role\": \"user\", \"content\": complaint_extraction_user_message}\n",
        "]\n",
        "\n",
        "# Let's call the API\n",
        "\n",
        "try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=chat_model_id,\n",
        "                messages=messages_for_complaints,\n",
        "                temperature=0.5, # Use a slightly higher temperature for more diverse summaries\n",
        "                max_tokens=200 # Adjust as needed for the length of expected complaints\n",
        "     )\n",
        "            complaints = response.choices[0].message.content\n",
        "            print(complaints)\n",
        "\n",
        "except Exception as e:\n",
        "            print(f\"Error extracting complaints: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tJzcCyyhgn_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customer complaints are very serious and could undermine the business significantly if no further corrective actions are taken\n",
        "This is a list of recommendations based on the top 5 customer complaints obtained from teh provided data:"
      ],
      "metadata": {
        "id": "Z5siai1PrWcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5.3 Business recommendations**\n",
        "\n",
        "The following are a list of recommended actions based on the business insights obtained from the data and that reflect the 5 main areas where customers have issues with."
      ],
      "metadata": {
        "id": "rD9z5pOMpvKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.3.1 **Delivery delays**\n",
        "\n",
        "5.3.1.1-**Enhance Predictive Analytics for ETAs**  \n",
        "- Implement or refine AI-driven ETA (Estimated Time of Arrival) models that incorporate real-time traffic data, weather conditions, historical delivery performance, driver availability, and typical handling times. This allows for more realistic and accurate delivery promises from the outset.  \n",
        "- Build realistic time buffers into delivery promises, especially for complex or less predictable routes, rather than over-optimizing for speed and risking missed deadlines.  \n",
        "\n",
        "5.3.1.2-**Optimize Routing and Fleet Management**  \n",
        "- Invest in dynamic route optimization software that can re-calculate and adjust routes in real-time based on unexpected events (e.g., traffic jams, sudden road closures, new urgent pickups/deliveries).  \n",
        "- Utilize advanced fleet management systems to monitor driver location, vehicle status, and performance, enabling dispatchers to intervene proactively if delays are anticipated.  \n",
        "\n",
        "5.3.1.3-**Proactive and Transparent Communication**  \n",
        "- Establish a system for automated, real-time notifications (SMS, email, app push) to customers regarding shipment status updates, including immediate alerts for any unforeseen delays, along with revised ETAs and clear reasons for the delay.  \n",
        "- Provide customers with robust, self-service tracking portals that offer granular updates and, crucially, display the most current predicted delivery window.  \n",
        "- Follow up with on customers to assess improvements are hear suggestions.  \n",
        "\n",
        "5.3.1.4-**Improve Last-Mile Efficiency**  \n",
        "- Explore strategies like micro-hubs or urban distribution centers to reduce travel time and congestion in dense delivery areas.  \n",
        "- Ensure drivers are equipped with efficient navigation tools and communication devices to report issues and receive real-time support.  \n",
        "\n",
        "5.3.1.5-**Identify and Address Root Causes**  \n",
        "- Conduct thorough data analysis on all late deliveries to identify recurring patterns (e.g., specific routes, times of day, types of goods, loading/unloading inefficiencies at certain points, specific internal processes) and implement targeted corrective actions.  \n",
        "- Gather post-delivery feedback specifically on timeliness to uncover customer-centric reasons for dissatisfaction.  \n",
        "- Meet with company drivers to get feedback of the issues they face, and establish goals and incentives for percentage of successful deliveries.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-LRjuohfw5-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.3.2 **Poor Customer Service**\n",
        "\n",
        "5.3.2.1-**Elevate Staff Training & Development**  \n",
        "\n",
        "- Intensive Soft Skills Training: Implement mandatory and ongoing training modules focused on active listening, empathy, de-escalation techniques, professional communication, and maintaining a positive demeanor even with difficult customers. Role-playing scenarios can be highly effective.  \n",
        "\n",
        "- Deep Product & Process Knowledge: Ensure all customer service representatives (CSRs) possess a thorough understanding of all services, policies, and common operational procedures, enabling them to provide accurate and comprehensive assistance.  \n",
        "\n",
        "- Empowerment & Problem-Solving: Train CSRs to identify root causes of issues quickly and empower them with the necessary authority and tools to resolve common problems during the first contact, minimizing transfers and follow-ups.\n",
        "\n",
        "5.3.2.2-**Optimize Responsiveness and Accessibility**  \n",
        "\n",
        "- Multi-Channel Strategy: Ensure seamless and consistent service across all customer contact points, including phone, email, live chat, and social media, with clear Service Level Agreements (SLAs) for response times.\n",
        "\n",
        "- AI-Powered Automation for Tier 0: Deploy intelligent chatbots or IVR (Interactive Voice Response) systems to handle routine inquiries (e.g., tracking updates, FAQs) instantly, freeing human agents to focus on more complex, high-value issues.   \n",
        "\n",
        "- Staffing & Scheduling Optimization: Use historical data and forecasting to ensure adequate staffing levels during peak hours, minimizing wait times across all channels.   \n",
        "\n",
        "5.3.2.3-**bEnhance Resolution Capabilities**  \n",
        "\n",
        "- Centralized Knowledge Base: Provide CSRs with an easily accessible, comprehensive, and regularly updated knowledge base containing solutions to common and complex issues.  \n",
        "\n",
        "- Integrated CRM System: Implement or upgrade your CRM to give agents a 360-degree view of customer history, previous interactions, and shipment details, eliminating the need for customers to repeat information.  \n",
        "\n",
        "- Clear Escalation Paths: Establish well-defined and efficient protocols for escalating complex or unresolved issues, ensuring smooth transitions and timely resolution by specialized teams.\n",
        "\n",
        "5.3.2.4-**Implement Robust Quality Assurance & Feedback Loops**  \n",
        "\n",
        "- Regular Interaction Monitoring: Systematically monitor calls, chat transcripts, and email communications for adherence to quality standards, professionalism, and effectiveness. Provide constructive feedback and coaching to agents.  \n",
        "\n",
        "- Customer Feedback Mechanisms: Deploy post-interaction surveys (e.g., CSAT, NPS) and actively solicit feedback to identify specific pain points, measure satisfaction with resolution, and pinpoint areas for agent-specific or process-wide improvement."
      ],
      "metadata": {
        "id": "e_jtw1Lb4-k3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.3.3 **Damaged Packages**\n",
        "\n",
        "This issues involves two stages: Shipper practices and Internal Handling Protocols\n",
        "\n",
        "5.3.3.1 **Enhance Shipper Education & Support on Packaging**  \n",
        "\n",
        "- Comprehensive Digital Guides: Develop user-friendly online guides, video tutorials, and interactive tools demonstrating best practices for packaging various item types (fragile, heavy, liquids) with specific material recommendations.  \n",
        "\n",
        "- Offer Premium Packaging Services/Materials: Provide an option for customers to purchase high-quality, pre-approved packaging materials or even opt for professional packing services at an additional cost, reducing reliance on inadequate shipper-provided packaging.  \n",
        "\n",
        "- Pre-shipment Advisory: For high-value or unusual shipments, offer a consultation service where customers can get expert advice on optimal packaging solutions.  \n",
        "\n",
        "5.3.3.2 **Strengthen Internal Handling Protocols & Training**\n",
        "\n",
        "- Mandatory Handling Training: Conduct rigorous, recurrent training for all personnel involved in the package journey (sorters, loaders, drivers) on proper lifting techniques, careful stacking, fragile item recognition, and minimizing impacts.\n",
        "\n",
        "- Automated System Optimization: Regularly audit and maintain automated sorting and conveyance systems to ensure they handle packages gently, reducing damage from drops, crushes, or impacts within facilities.  \n",
        "\n",
        "- Secure Loading Procedures: Implement strict protocols for loading vehicles, ensuring proper weight distribution, secure bracing of cargo, and safe stacking to prevent shifting and damage during transit.  \n",
        "\n",
        "5.3.3.3 **Implement Quality Control & Pre-emptive Checks**\n",
        "\n",
        "- Initial Packaging Inspection: Empower and train pickup drivers or drop-off point staff to perform a quick visual assessment of external packaging. If clearly inadequate for the contents (e.g., box too weak, obvious liquid leaks), advise the customer and potentially refuse the shipment without proper re-packaging.  \n",
        "\n",
        "- Spot Checks at Sortation Centers: Introduce random or targeted quality control checks at key transfer points to identify poorly packaged items that may require re-packaging by your staff (potentially with associated fees) before further transit.  \n",
        "\n",
        "- Damage Reporting Tools: Equip drivers with tools to quickly document any package damage discovered before delivery, including photo evidence and immediate reporting back to the hub.  \n",
        "\n",
        "5.3.3.4 **Data-Driven Damage Prevention**\n",
        "\n",
        "- Analyze Claim Data: Systematically collect and analyze data from all damage claims to identify patterns. Look for common transit lanes, specific types of items, particular sorting centers, or even specific vehicle types associated with higher damage rates.  \n",
        "\n",
        "- Iterative Process Improvement: Use these insights to implement targeted operational adjustments, new equipment, or revised handling instructions to address the root causes of damage.\n"
      ],
      "metadata": {
        "id": "rua9W5mD9KWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.3.4 **Inaccurate Tracking System**\n",
        "\n",
        "5.3.4.1 **Enhance Data Capture and Integration at Every Touchpoint**\n",
        "\n",
        "- Advanced Scanning Technology: Upgrade to state-of-the-art scanning equipment at all processing points (pickup, sorting, loading, transit, delivery) to ensure precise and instantaneous capture of package status updates.\n",
        "\n",
        "- Driver Mobile App Optimization: Equip drivers with intuitive, reliable mobile applications that facilitate accurate and real-time scanning and status updates (e.g., \"Out for Delivery,\" \"Delivery Attempted,\" \"Delivered\"), including GPS timestamps for location verification.\n",
        "\n",
        "- Seamless System Integration: Ensure all operational systems (warehouse management, fleet management, dispatch, customer service) are fully integrated to allow for immediate data synchronization and prevent information silos.\n",
        "\n",
        "5.3.4.2 **Ensure Real-time Data Transmission and System Reliability**\n",
        "\n",
        "- Instant Data Sync: Implement a system architecture that pushes data from scan points directly to the central tracking database in real-time, minimizing any lag between an event occurring and its reflection in the tracking system.\n",
        "- Robust Network Infrastructure: Invest in reliable internet connectivity and network systems across all facilities and mobile units to guarantee consistent data flow without interruptions.\n",
        "- Cloud-Based Scalability: Utilize a cloud-native tracking platform that can scale to handle high volumes of concurrent updates and ensure uptime, providing customers with reliable access 24/7.\n",
        "\n",
        "5.3.4.3 **Improve Clarity and User Experience of Tracking Information**\n",
        "\n",
        "- Simplified Status Definitions: Translate internal operational codes into clear, plain-language status updates for customers (e.g., \"In Transit,\" \"Ready for Pickup,\" \"Delivery Attempted – Customer Not Home\").\n",
        "- Dynamic Estimated Delivery Windows: Display evolving estimated delivery windows or times on the tracking portal that adjust based on real-time factors like traffic or delays, rather than just static dates.\n",
        "- Proactive Explanations for Exceptions: For any unusual statuses or significant delays, provide concise, automated explanations directly within the tracking interface (e.g., \"Delay due to inclement weather,\" \"Customs clearance in progress\").\n",
        "\n",
        "5.3.4.4 **Implement Rigorous Monitoring and Auditing**\n",
        "\n",
        "- Automated Discrepancy Alerts: Set up internal monitoring tools that automatically flag inconsistencies, missed scans, or packages remaining in the same status for an unusual duration, prompting immediate investigation by operations teams.\n",
        "- Regular Data Audits: Conduct routine audits of tracking data to identify common points of error or delay in information flow, allowing for targeted process improvements or technology upgrades.\n",
        "- Customer Feedback on Tracking: Actively solicit feedback on the clarity and accuracy of the tracking system through surveys to pinpoint areas needing improvement from the user's perspective."
      ],
      "metadata": {
        "id": "Y6d8V5O0BUK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.3.5 **Hidden Fees and Misleading Pricing**\n",
        "\n",
        "5.3.5.1 **Implement a Comprehensive, All-Inclusive Quoting System**\n",
        "\n",
        "- Real-time, Final Pricing: Develop an online quoting tool that provides an upfront, all-inclusive estimated cost at the point of inquiry, encompassing all standard surcharges (e.g., fuel, residential delivery, remote area fees, weekend delivery) based on the input details (weight, dimensions, origin, destination, service level).\n",
        "\n",
        "- Transparent Breakdown: Within the quote, clearly itemize and explain every component of the price (base rate, all surcharges, taxes, and any value-added service fees), even if only the total is initially highlighted.\n",
        "\n",
        "- Dynamic Calculation: Ensure the system accurately calculates potential fees for specific services or destinations, adjusting in real-time as customers input details.\n",
        "\n",
        "5.3.5.2 **Proactive and Clear Disclosure of Potential Additional Charges**\n",
        "\n",
        "- Prominent Warnings: Clearly list and explain common scenarios that might trigger post-quote additional charges (e.g., re-delivery attempts, address corrections due to customer error, customs duties/taxes for international shipments, special handling for undeclared oversized/fragile items). Display these warnings at relevant stages (e.g., during booking, in confirmation emails).\n",
        "\n",
        "- Pre-shipment Verification: If possible, implement a system to verify key shipment characteristics (e.g., weight, dimensions) early in the process. If discrepancies are found that affect pricing, inform the customer before the final invoice is issued, offering a chance to adjust or cancel.\n",
        "\n",
        "- Accessible Terms: Ensure your pricing terms and conditions, including all potential fees, are easily findable on your website and written in clear, unambiguous language.\n",
        "\n",
        "5.3.5.3 **Standardize and Justify All Surcharges**\n",
        "\n",
        "- Clear Surcharge Explanations: Provide concise, simple explanations for why each surcharge exists (e.g., \"Fuel Surcharge: Reflects the variable cost of fuel for transportation,\" \"Peak Season Surcharge: Applied during periods of unusually high demand to manage network capacity\").  \n",
        "\n",
        "- Regular Review and Rationalization: Periodically review all existing surcharges to ensure they remain relevant, fair, and competitive. Eliminate any obsolete or confusing fees.  \n",
        "\n",
        "5.3.5.4 **Improve Invoicing Clarity and Dispute Resolutio**\n",
        "\n",
        "- Detailed, Itemized Invoices: Issue invoices that precisely match the quoted breakdown and clearly show all charges applied, linking them directly to the services provided.  \n",
        "\n",
        "- Automated Discrepancy Flagging: Implement internal systems that automatically flag significant differences between the initial quote and the final charge, prompting internal review and proactive communication with the customer before billing.  \n",
        "\n",
        "- Streamlined Dispute Process: Establish a clear, accessible, and prompt process for customers to dispute unexpected charges, ensuring rapid investigation and transparent resolution."
      ],
      "metadata": {
        "id": "FD1Qd1vTBT79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q32adQCNK-z9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Final Note**\n",
        "\n",
        "The above described suggestions should be discussed as part of the short, medium, and long term company improvement plan and performance goal settings for ExpressWay Logistics. The implementation of said recommendations is usbject to the company's priorities and availability of required resources, such as financial, managerial, administraive and  human resources."
      ],
      "metadata": {
        "id": "oPFyrZ-PHQm_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}